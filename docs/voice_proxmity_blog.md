## Introduction
Humans are generally effective at determining if two voices are similar or are different. However, quantifying this proximity between voices has proven to be difficult. Developing an algorithm that could “plot” voices of a population of individuals such that samples are closer to similar voices and farther from different voices could enable a wide range of research - from voice cryptography, to better understanding how the brain processes speaker identity, to automatically diagnosing various voice disorders. This could also help with the reproducibility of studies that use voice as a stimuli, as it would enable better quantification of how subsets of voices are different instead of relying on purely subjective measures.

There are a few criteria for success when trying to quantify voice proximity. First, samples from one speaker should clump together when we plot them, which would represent that speaker’s “identity” or “voice print”. Second, the distances between speakers should be relatively stable. If a speaker is removed from a population, the distance between the remaining speakers should remain relatively similar. If a set of high-dimension features is used so we can better visualize the relationship between speakers in two or three dimensions instead of tens or hundreds, the local and global structure of the high-dimension features space should be preserved. This implies that the proximity between speakers has some real-world meaning. Third, some fundamental subjective tests should match the computed results. For example, the fundamental frequency of a voice has a major impact on the perceived identity of said voice, so we would expect gradient among the samples that maps to the F0. While not a criteria, it would be interesting to see how the effectiveness of a voice proximity algorithm changes as the voice recordings from participants are subdivided into smaller and smaller segments of speech to try to understand how much voice is needed to be able to identify a voice from amongst a population of other voices. 

There are multiple ways to approach this problem. We could try to hand-pick signal processing features of voice (such as aforementioned F0) and see how those help us tease apart a voice proximity space. This is beneficial since hundreds of features have been developed and studied to measure elements of voice quality and their connection to physiology is often understood, making our features understandable and potentially generalizable. However, hand-picking features could be an inefficient process given the number of features to filter through and the diversity in which these features are even processed.

Another approach could leverage machine learning. To take a specific example which we leverage in this article, pyannote-audio is a set of tools that have been developed for speaker diarization. Essentially, the model have been trained to separate voices that are mixed in a recording. In order to separate voices in this way, the model has found features that enable multiple voices in one signal to be identified as different and teased apart. In other words, we already have an algorithm that separate “different” voices, and now we could explore those features to understand whether the relative distance between these voices means anything per our criteria above on a good algorithm for voice proximity, or if it just means different. However, these features are essentially the product of black-box engineering. As they stand, they are not interpretable in terms of known features of a voice, so explaining how the algorithm works or why it works that way becomes a challenge.

For our work, we combined both approaches. Algorithms that are good at diarization of speakers provide an ideal foundation to start to develop a voice proximity algorithm. The analysis in the “machine learning space” can then be compared to an analysis of known signal processing features to try to understand which of these features is represented by the model.

## Methods/Results
- Split recordings into small utterances
- Extract embeddings that enable diarization from each
- Calculate distances between speaker point clouds
- Leave one speaker out and measure the effect on the pairwise distances
- Reduce the dimensions of the embeddings to visualize the data in a low-dimension space (always critical to understanding what is happening)
- Quantify how well the low-dimension space preserves the local and global structure of the high dimension space. If we found this high dimension space to be meaningful, we don’t want to lose this just to create pretty plots
- Repeat the leave-one-out analysis. This would show us that we can simplify our voice proximity into a meaningful-low dimension space as well, which can simplify and speed up our analysis while knowing how good we are and what we lost from the high dimension space. Is this tradeoff ok?
- If this works, look at correlations to known features such as egemaps. How are these features represented in the high dimension space, or the low dimension space, if at all. What known features are driving this quantification of voice proximity. This is new knowledge and can be compared to other findings and hypothesis

## Conclusion
This is what we found! It works
These are gaps and limitations!
This is future work!
