{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c646a52e-eb75-4b16-8fcd-5796bbcda9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "\n",
    "from pyannote.audio import Inference\n",
    "from pyannote.audio.utils.signal import Binarize, Peak\n",
    "from pyannote.core import Segment, notebook, SlidingWindowFeature, timeline, Timeline\n",
    "\n",
    "from skimage.measure import block_reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fb3ea1-8ff7-4853-9aab-e077cd8d7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyannote_extract_embs(one_file, one_diar, window_type=\"sliding\"):\n",
    "    \n",
    "    if window_type == \"sliding\":\n",
    "        emb = Inference(\"pyannote/embedding\", \n",
    "                          window=\"sliding\",\n",
    "                          duration=3.0, step=1.0)\n",
    "    elif window_type == \"whole\":\n",
    "        emb = Inference(\"pyannote/embedding\", window=\"whole\")\n",
    "    \n",
    "    #using pyannote audio pretrained model tutorial as is: https://github.com/pyannote/pyannote-audio/tree/master/tutorials/pretrained/model\n",
    "    #Main change from tutorial - don't calculate the means of the embeddings extracted from each 500ms speech turn.\n",
    "    \n",
    "    #one_file: .wav file in the format the model expects\n",
    "    #diar: diarization map with pyannote timeline to just extract based on intended speaker\n",
    "    \n",
    "    emb_from_sample = []\n",
    "    \n",
    "    # obtain raw embeddings (as `pyannote.core.SlidingWindowFeature` instance)\n",
    "    #one_file is a .wav\n",
    "    embeddings = emb(one_file)\n",
    "\n",
    "    #We only work of long (> ts) speech turns. Default is 2s from the tutorial.\n",
    "    # DIAR only has speech turns from desired participant\n",
    "    #t = 2\n",
    "    #long_turns = Timeline(segments=[s for s in one_diar if s.duration > t])\n",
    "\n",
    "    #for each long turn of >t seconds long, extract each 500ms segment of embeddings \n",
    "    if one_diar==None:\n",
    "        emb_from_sample.append(embeddings)\n",
    "    else:\n",
    "        for segment in one_diar:\n",
    "            inter = embeddings.crop(segment, 'strict')\n",
    "            emb_from_sample.append(inter)\n",
    "            #Tutorial calculated the mean of all the embeddings from a 500ms segment, but we keep them all\n",
    "            #emb_from_sample.append(np.mean(inter, axis=0)\n",
    "    emb_from_sample = np.vstack(emb_from_sample) #vstack to get the list into numpy format with easy to understand #of embeddings X embedding values structure\n",
    "    return emb_from_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30fa4000-e049-4c85-8ef4-a69d45a110f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Downsample audio to 16kHz\n",
    "root_dir= \"/Users/rahulbrito/Documents/projects/infantvoice/data/Full_Readings/gasser_readings\"\n",
    "audio_files = os.path.join(root_dir,\"preprocessed_audios_dur3sec\")\n",
    "\n",
    "#make a folder with today's date for the downsampled audio\n",
    "down_sample_dir = os.path.join(root_dir,\"preprocessed_audios_dur3sec\")\n",
    "\n",
    "import glob\n",
    "all_files = glob.glob(down_sample_dir+'/*/*', recursive=True)\n",
    "all_files = [file for file in all_files if 'script' in file]\n",
    "\n",
    "one_file = all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7277f96c-4dc9-459f-b443-df4611e46bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulbrito/Documents/projects/infantvoice/data/Full_Readings/gasser_readings/preprocessed_audios_dur3sec/132/132_M_LPP_script_019.wav'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "825e4666-53aa-454a-b0ef-8fa2839d6a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pyannote_extract_embs(one_file, one_diar=None, window_type=\"whole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2db7207c-2a37-4e5c-b919-422720564579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065f0ea-274e-4fa4-b643-855c9de3db80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infantvoice",
   "language": "python",
   "name": "infantvoice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
