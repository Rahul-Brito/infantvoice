{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "246dd9cc-f412-4cb0-a3d2-f859b2a6ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import seed, shuffle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import json\n",
    "\n",
    "from matplotlib import pyplot as plt, lines\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import directed_hausdorff, euclidean, cosine, pdist\n",
    "\n",
    "from src import (\n",
    "    downsample as downsamp_audio, \n",
    "    embedding_extractor as ee, \n",
    "    dimension_reducer as dr, \n",
    "    distance_metrics as dm, \n",
    "    downsample as ds,\n",
    "    emb_manipulator as em\n",
    ")\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6871077-d552-4481-9141-9abeefa00840",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Downsample audio to 16kHz\n",
    "root_dir= \"/Users/rahulbrito/Documents/projects/infantvoice/data/Full_Readings/053122_new_participants/\"\n",
    "audio_files = os.path.join(root_dir,\"audio\")\n",
    "\n",
    "#make a folder with today's date for the downsampled audio\n",
    "down_sample_dir = os.path.join(root_dir,\n",
    "                               datetime.now().strftime('%Y_%m_%d') + '_16kHz')\n",
    "\n",
    "if not os.path.exists(down_sample_dir):\n",
    "    os.mkdir(down_sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d06200-eec8-4784-8183-294c249b2adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing023.wav\n"
     ]
    }
   ],
   "source": [
    "ds.downsamp_audio(audio_files, down_sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0aa5e5-4e4d-4185-9bd2-4b1cb87c6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Diarize audio to find when mother is speaking and visualize it to choose label\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "diar_dir = os.path.join(root_dir,'audio',\n",
    "                           datetime.now().strftime('%Y_%m_%d') + '_diarized')\n",
    "if not os.path.exists(diar_dir):\n",
    "    os.mkdir(diar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215b86b-db93-407a-b45b-d6f455bbb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(down_sample_dir):\n",
    "    if file.endswith('wav'):\n",
    "        diar = pipeline(os.path.join(down_sample_dir,file))\n",
    "        json.dump_to(diar, os.path.join(diar_dir, os.path.splitext(file)[0]+'.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9667181-1788-49bf-aa42-00646a6d7300",
   "metadata": {},
   "outputs": [],
   "source": [
    "diar_dir = '/Users/rahulbrito/Documents/projects/infantvoice/data/Full_Readings/053122_new_participants/2022_05_31_diarized'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "402f3a36-576b-4937-823e-27c70b533563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on output of diarization, manually make speaker map to set which speaker we want\n",
    "speaker_map = {'021':'SPEAKER_01', '022':'SPEAKER_00', 'BB003':'SPEAKER_00'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a7022c61-e2d9-458b-8048-f096a65cc5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diarized = {os.path.splitext(file)[0]:\n",
    "            json.load_from(os.path.join(diar_dir,file)).label_timeline(\n",
    "                speaker_map[os.path.splitext(file)[0]]) \n",
    "            for file in os.listdir(diar_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "967ae8ac-1c04-445a-9364-0994594089ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing022.wav\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "emb_dir = '/Users/rahulbrito/Documents/projects/infantvoice/data/embeddings'\n",
    "save_name = datetime.now().strftime('%Y_%m_%d') + '_emb.csv'\n",
    "pyannote_emb = ee.pyannote_extract_directory(down_sample_dir, diarized,emb_dir,save_name,save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c5f07d71-7219-4375-bc64-6784d04b7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['03016_diarized_pyv2.csv','2022_06_01_emb.csv']\n",
    "\n",
    "emb_unscaled = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    emb_unscaled = pd.concat([emb_unscaled,pd.read_csv(os.path.join(emb_dir,f), index_col=0)], ignore_index=True)\n",
    "\n",
    "#emb_down_unscaled = em.resample_data(emb,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "emb = pd.DataFrame(scaler.fit_transform(emb_unscaled.drop(columns='part_id')))\n",
    "emb['part_id'] = emb_unscaled.part_id.astype('string').to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176d7bdc-6966-4a4c-9951-60980cbba8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default\n",
    "#learning_rate = emb_down.shape[0]//12\n",
    "#perplexity = 30\n",
    "\n",
    "#max global structure\n",
    "perplexity = 135\n",
    "learning_rate = 200\n",
    "\n",
    "#default\n",
    "#n_neighbors = 10\n",
    "#dist=0.1\n",
    "\n",
    "#max global structure\n",
    "n_neighbors=14\n",
    "dist=10E-3\n",
    "\n",
    "###Run dimension reduction with different methods on the embeddings to establish a baseline before leaving any participants out\n",
    "#Using tsne with perplexity=30 (default) since N/100<30, and learning rate of n/12. Random seed is 42 so the result is the same each time\n",
    "emb_tsne = dr.run_tsne(emb_down, perplexity = perplexity, init='pca', learning_rate=learning_rate)\n",
    "\n",
    "#run umap with default parameters\n",
    "emb_umap = dr.run_umap(emb_down, n_neighbors=n_neighbors, dist=dist)\n",
    "\n",
    "\n",
    "labels = emb_down.part_id\n",
    "\n",
    "#PACMAP for comparison\n",
    "emb_pacmap = dr.run_pacmap(emb_down)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "infantvoice",
   "language": "python",
   "name": "infantvoice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
